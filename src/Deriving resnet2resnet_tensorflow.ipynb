{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derviving and implmenting resnet2resnet transformations\n",
    "\n",
    "TODO: Describe zero initializations\n",
    "\n",
    "TODO: Define the two convolutions \n",
    "\n",
    "TODO: describe extending a kernel with C1 filters, and extending with 2\\*C2 filters. Where we initialize to W and -W\n",
    "\n",
    "TODO: Initialization of a new identity layer (net2deeper transform) is the same as a net2wider transform, with C1 = 0.\n",
    "\n",
    "TODO: Describe why this doesn't work, because it only works with odd activation functions (i.e. tanh). This is obviously too restrictive.\n",
    "\n",
    "TODO: describe initializing with W and W, and then adding the $beta$ and $-beta$ on a 1x1 conv\n",
    "\n",
    "TODO: describe the additional benefit of the 1x1 convs to provide a consistent dimension on the output\n",
    "\n",
    "TODO: Add our final diagrams to help explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "Here's just some code to make matplotlib libraries etc play nice with jupyter notebooks. (So we just run the below code and don't worry about how it works).\n",
    "\n",
    "This will setup plotting how we want it, make matplotlib and seaborn figures run inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Some imports\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from __future__ import print_function\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Seaborn config\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the kernel\n",
    "\n",
    "Here we provide the code to zero initialize a module. We begin by defining the normal (Xavier and He) initializations for numpy arrays, and extend them to provide initializations for \"widennings\" of convolutional filters. The aim for this portion of the notebook is to take a set of convolutional filters F, and initialize further filters, W, and return a concatenated set of filters, [F;W;W]. (N.B. we use ';' for concatenation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs231n/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import the tensorflow and numpy libraries we need now\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_xavier_initialize(filter_shape, override_output_channels=None):\n",
    "    \"\"\"\n",
    "    Initialize a convolutional filter, with shape 'filter_shape', according to \"He initialization\".\n",
    "    The weight for each hidden unit should be drawn from a normal distribution, with zero mean and stddev of \n",
    "    sqrt(2/(n_in + n_out)).\n",
    "    \n",
    "    This is the initialization of choice for layers with non ReLU activations.\n",
    "    \n",
    "    The filter shape should be [width, height, input_channels, output_channels]. So here, n_in = input_channels \n",
    "    and n_out = width * height * output_channels.\n",
    "    \n",
    "    When \"widening\" an filter, from C1 output filters, to C1 + 2*C2 filters, then we want to initialize the \n",
    "    additional 2*C2 layers, as if there are C1+2*C2 filters in the output, and therefore we provide the \n",
    "    option to override the number of output filters.\n",
    "    \n",
    "    :param filter_shape: THe shape of the filter that we want to produce an initialization for\n",
    "    :param override_output_channels: Override for the number of output filters in the filter_shape (optional)\n",
    "    :return: A numpy array, of shape 'filter_shape', randomly initialized according to He initialization.\n",
    "    \"\"\"\n",
    "    width, height, in_channels, out_channels\n",
    "    if override_output_channels is not None:\n",
    "        out_channels = override_output_channels\n",
    "    filter_shape = (width, height, in_channels, out_channels)    \n",
    "    \n",
    "    scale = np.sqrt(2.0 / (in_channels + width*height*out_chanels))\n",
    "    return scale * np.random.randn(*filter_shape).astype(np.float32) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def conv_he_initialize(filter_shape):\n",
    "    \"\"\"\n",
    "    Initialize a convolutional filter, with shape 'filter_shape', according to \"Xavier initialization\".\n",
    "    Each weight for each hidden unit should be drawn from a normal distribution, with zero mean and stddev of \n",
    "    sqrt(2/n_in).\n",
    "    \n",
    "    This is the initialization of choice for layers with ReLU activations.\n",
    "    \n",
    "    The filter shape should be [width, height, input_channels, output_channels]. So here, n_in = input_channels.\n",
    "    \n",
    "    As the initization only depends on the number of inputs (the number of input channels), unlike Xavier \n",
    "    initialization, we don't need to be able to override the number of output_channels.\n",
    "    \n",
    "    :param filter_shape: THe shape of the filter that we want to produce an initialization for\n",
    "    :return: A numpy array, of shape 'filter_shape', randomly initialized according to He initialization.\n",
    "    \"\"\"\n",
    "    in_channels = filter_shape[2]\n",
    "    scale = np.sqrt(2.0 / in_channels)\n",
    "    return scale * np.random.randn(*filter_shape).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_filter_with_repeated_weights(extending_filter_shape, existing_filter=None, init_type='He'):\n",
    "    \"\"\"\n",
    "    We want to initialize a filter with appropriately initialized weights.\n",
    "    \n",
    "    Let F be the 'existing_filter', with shape [W,H,I,C1]. Let extending_filter_shape be [W,H,I,2*C2]. \n",
    "    If the value for 2*C2 is odd or non-positive, then it's an error, if the first the\n",
    "    We then want to initialize E of shape [W,H,I,C2], according to the given initialization, \n",
    "    and then we want to return the concatenation [F;E;E].\n",
    "    \n",
    "    To make a fresh/new filter, with repeated weights, let 'existing_filter' be None, and it will \n",
    "    return just [E;E], as F is \"empty\".\n",
    "    \n",
    "    :param extending_filter_shape: The shape of the new portion of the filter to return. I.e. [W,H,I,2*C2]\n",
    "    :param existing_filter: If not None, it must have shape [W,H,I,C1], this is the existing filter.\n",
    "    :param init_type: The type of initialization to use\n",
    "    :return: A filter, extended by 2*C2 channels. I.e. the filter [F;E;E]\n",
    "    \"\"\"\n",
    "    # Unpack params input\n",
    "    W, H, I, twoC2 = extending_filter_shape\n",
    "    C2 = twoC2 // 2\n",
    "    C1 = 0 if existing_filter is None else existing_filter.shape[3]\n",
    "    \n",
    "    # Error checking\n",
    "    if twoC2 % 2 != 0:\n",
    "        # TODO: log a descriptive error in final implementation\n",
    "        raise Exception()\n",
    "    elif existing_filter is not None and (W != existing_filter.shape[0] \n",
    "        or H != existing_filter.shape[1] or I != existing_filter.shape[2]):\n",
    "        # TODO: log a descriptive error in final implementation\n",
    "        raise Exception()\n",
    "    \n",
    "    # Canvas for the new numpy array that we want to return, and copy existing filter weights\n",
    "    canvas = np.zeros((W,H,I,C1 + twoC2)).astype(np.float32)\n",
    "    if existing_filter is not None:\n",
    "        canvas[:,:,:,:C1] = existing_filter\n",
    "\n",
    "    # Initialize the new weights, and copy that into the canvas (twice)\n",
    "    new_channels_weights = None\n",
    "    if init_type == 'He':\n",
    "        new_channels_weights = conv_he_initialize((W,H,I,C2))\n",
    "    elif init_type == 'Xavier':\n",
    "        new_channels_weights = conv_xavier_initialize((W,H,I,C2), C1+twoC2)\n",
    "    else:\n",
    "        # TODO: log a descriptive error in final implementation\n",
    "        raise Exception()\n",
    "    \n",
    "    canvas[:,:,:,C1:C1+C2] = new_channels_weights\n",
    "    canvas[:,:,:,C1+C2:C1+twoC2] = new_channels_weights\n",
    "\n",
    "    # Done :)\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "Now that we can extend a filter, with weights F, to [F;W;W], we use it to build our zero module using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_filter_shape_v1(inputs, output_channels):\n",
    "    \"\"\"\n",
    "    (Helper function for 'init_vero_module_v1').\n",
    "    Computes the shape of the filter to be used, give the number of 'output_channels' we wish to have\n",
    "    Also provides error checking!\n",
    "    \n",
    "    As v1 is just prototyping, we're just going to always set a filter size of 4x4.\n",
    "    \n",
    "    Also in v1, we completely ignore padding and stride, and let tf handle appropriate padding so that \n",
    "    the output width and height is the same as the width and height input.\n",
    "    \n",
    "    We check that the numbe of output channels is even, because we want the output to be symmetric in \n",
    "    the output channels. We also check that the number of output channels is at least the number of \n",
    "    input channels, because we want to add residual connections, and we can't really do that unless \n",
    "    in_channels <= out_channels\n",
    "    \"\"\"\n",
    "    # 0th item is batch size\n",
    "    width, height, in_channels = inputs.get_shape().as_list()[1:] \n",
    "    \n",
    "    if output_channels % 2 != 0:\n",
    "        # TODO: log a descriptive error in final implementation\n",
    "        raise Exception()\n",
    "    if in_channels > output_channels:\n",
    "        # TODO: log a descriptive error in final implementation\n",
    "        raise Exception()\n",
    "    \n",
    "    return (4,4, in_channels, output_channels)\n",
    "    \n",
    "        \n",
    "\n",
    "def init_zero_module_v1(inputs, output_channels, add_max_pool=False, add_batch_norm=True, noise_ratio=1e-3, \n",
    "                        scope=\"\"):\n",
    "    \"\"\"\n",
    "    Initializes a convolutional layer such that if there are 2N filters output, then the output of filter \n",
    "    0 < i <= N is equal to output of filter N+i.\n",
    "\n",
    "    For now, we use 4x4 convolutions, with a stride of 1, currently 2x2 max pooling, and only relu \n",
    "    non-linearity. Seeing at this is a v1 prototype!! (Note that we're anyway going to extend this from \n",
    "    one filter size to many filter sizes, as in inception networks).\n",
    "    \n",
    "    Also, for the the moment, we set the input and output for the 1x1 convolution at the end to use \n",
    "    'output_channels' number of channels. We can change this later to two seperate numbers, but it's \n",
    "    uninteresting (for now).\n",
    "    \n",
    "    In later versions, we should also allow the options for different activation functions.\n",
    "\n",
    "    Outline of the computation that we perform:\n",
    "    0. (Optionally) reduce the input dimensions, using a max pool\n",
    "    1. Input shape of [A,A,D], and desired output shape is [A,A,2K]\n",
    "    2. Make a filter, with shape [4,4,D,2K], where [4,4,D,:K] is identitcal to [4,4,D,K:], \n",
    "        that is: set the weights for [:,:,:,K+j] equal to  [:,:,:,j]\n",
    "    3. Add noise, at an appropriate scale, to break symmetry\n",
    "    4. Add batch norm, if we want to\n",
    "    5. Apply our non-linearity, \n",
    "    6. By the symmetry introduced, the output after the non-linearity should be of the form [W'; W'] \n",
    "        (that is, still symmetric)\n",
    "    7. Use a 1x1 convolution to use the symmetry to make the output from the previous layer cancel out \n",
    "        (note that this also allows use to provide dimensionality reduction, and, output a consistent number \n",
    "        of filters, helping to keep everything modular and all layers independent. (The next layer doesn't \n",
    "        need to change its input shape)).\n",
    "    \n",
    "    :param inputs: input to the zero module\n",
    "    :param num_output_filters: the number of output channels to have in the output\n",
    "    :param add_max_pool: should we add a max pool before anything else?\n",
    "    :param add_batch_norm: should we add a batch norm layer before the activation?\n",
    "    :param noise_ratio: the ratio (with respect to the max value in the filter) of noise to add\n",
    "    :param scope: tensorflow variable scoping\n",
    "    :return: output from the zero module\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        # If max pooling, we apply that first\n",
    "        max_pool_outputs = inputs\n",
    "        if add_max_pool:\n",
    "            max_pool_outputs = tf.layers.max_pooling2d(inputs, pool_size=2, strides=2)\n",
    "\n",
    "        # Compute [A,A,D,2K], and initilize a filter using 'init_filter_with_repeated_weights'\n",
    "        filter_shape = _compute_filter_shape_v1(inputs, output_channels)\n",
    "        filter_init = init_filter_with_repeated_weights(filter_shape, init_type='He')\n",
    "\n",
    "        # Actually make the filter tf variable, and make the convolutional layer\n",
    "        filtr = tf.Variable(filter_init, name=scope+\"/conv_filter\")\n",
    "        stride = 1\n",
    "        conv_outputs = tf.nn.conv2d(input = inputs,\n",
    "                                    filter = filtr,\n",
    "                                    strides = [1,stride,stride,1],\n",
    "                                    padding = \"SAME\")\n",
    "\n",
    "        # Apply batch norm\n",
    "        batch_norm_outputs = conv_outputs\n",
    "        if add_batch_norm:\n",
    "            batch_norm_outputs = tf.contrib.layers.batch_norm(conv_outputs)\n",
    "\n",
    "        # apply relu\n",
    "        relu_output = tf.nn.relu(batch_norm_outputs)\n",
    "        \n",
    "        # init the 1x1 reduction convolution filter, negating the second half of the *inputs*\n",
    "        # for now, input_channels = output_channels (= output_channels from zero module) because lazyness\n",
    "        half_filters = output_channels // 2\n",
    "        filter_shape = (1, 1, output_channels, output_channels)\n",
    "        filter_init = init_filter_with_repeated_weights(filter_shape, init_type='He')\n",
    "        filter_init[:, :, half_filters:, :] = - filter_init[:, :, :half_filters, :]\n",
    "        \n",
    "        # Add noise to filter, as a ratio of the maximum entry from filter_init\n",
    "        max_entry = np.max(np.abs(filter_init))\n",
    "        noise_scale = noise_ratio * max_entry\n",
    "        filter_init += noise_ratio * np.random.randn(*filter_shape).astype(np.float32)\n",
    "        \n",
    "        # apply the 1x1 convolution. With the symmetry above, this will produce a(n approximately) zero output\n",
    "        reduction_filter = tf.Variable(filter_init, name=scope+\"/conv_reduction_filter\")\n",
    "        stride = 1\n",
    "        output = tf.nn.conv2d(input = relu_output,\n",
    "                              filter = reduction_filter,\n",
    "                              strides = [1,stride,stride,1],\n",
    "                              padding = \"SAME\")\n",
    "        \n",
    "        # return, caching all of the intermediate variables, to use when \"widening\" this layer\n",
    "        cache = [max_pool_outputs, filtr, batch_norm_outputs, relu_output, reduction_filter, output]\n",
    "        return output, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checking what we've done so far\n",
    "\n",
    "Now, lets run a few tests, as follows, to sanity check that everything that we've done so far works how we intended it to.\n",
    "1. Set the noise to zero, and check that the output is zero for *any* input. Initially do this without batch norm.\n",
    "2. Do the same, but with batch norm added.\n",
    "2. Set the noise to a small variable, and check that the output is approximately zero (and non-symmetric).\n",
    "4. Repeat test 3, with batch norm \n",
    "3. To gauge the effect that noise has, run the previous test, with a number of noise ratios.\n",
    "5. Repreate test 5, with batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Zero initialized module, no batch norm, zero noise, check with 10 different inputs that the output is zero.\n",
      "\tOn iter 0, average output value magnitude is 0.000001\n",
      "\tOn iter 1, average output value magnitude is 0.000001\n",
      "\tOn iter 2, average output value magnitude is 0.000003\n",
      "\tOn iter 3, average output value magnitude is 0.000002\n",
      "\tOn iter 4, average output value magnitude is 0.000002\n",
      "\tOn iter 5, average output value magnitude is 0.000002\n",
      "\tOn iter 6, average output value magnitude is 0.000002\n",
      "\tOn iter 7, average output value magnitude is 0.000002\n",
      "\tOn iter 8, average output value magnitude is 0.000001\n",
      "\tOn iter 9, average output value magnitude is 0.000001\n",
      "\tThe output from the final run was exactly this: \n",
      "\t[[[[-2.59460080e-08 -4.53532465e-08 -2.59460080e-08 -4.53532465e-08]\n",
      "\t   [-1.93779215e-08 -3.48165941e-10 -1.93779215e-08 -3.48165941e-10]\n",
      "\t   [ 8.31184366e-09  1.76928523e-07  8.31184366e-09  1.76928523e-07]\n",
      "\t   [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\t\n",
      "\t  [[ 2.96967784e-09  6.15380635e-09  2.96967784e-09  6.15380635e-09]\n",
      "\t   [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "\t   [ 8.81810536e-09 -5.69142351e-08  8.81810536e-09 -5.69142351e-08]\n",
      "\t   [ 1.85887785e-08 -2.19642082e-08  1.85887785e-08 -2.19642082e-08]]\n",
      "\t\n",
      "\t  [[-7.45279394e-10  5.72190402e-08 -7.45279394e-10  5.72190402e-08]\n",
      "\t   [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "\t   [-1.16078525e-08 -2.17341594e-08 -1.16078525e-08 -2.17341594e-08]\n",
      "\t   [-2.33055459e-08  8.80244002e-08 -2.33055459e-08  8.80244002e-08]]\n",
      "\t\n",
      "\t  [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "\t   [-8.40877590e-10 -1.58707358e-09 -8.40877590e-10 -1.58707358e-09]\n",
      "\t   [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "\t   [ 6.49245990e-10 -7.16590876e-09  6.49245990e-10 -7.16590876e-09]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test 2: same as test 1, with batch norm added, and max pooling added\n",
      "WARNING:tensorflow:From /anaconda/envs/cs231n/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "\tOn iter 0, average output value magnitude is 0.000001\n",
      "\tOn iter 1, average output value magnitude is 0.000001\n",
      "\tOn iter 2, average output value magnitude is 0.000001\n",
      "\tOn iter 3, average output value magnitude is 0.000001\n",
      "\tOn iter 4, average output value magnitude is 0.000001\n",
      "\tOn iter 5, average output value magnitude is 0.000001\n",
      "\tOn iter 6, average output value magnitude is 0.000001\n",
      "\tOn iter 7, average output value magnitude is 0.000001\n",
      "\tOn iter 8, average output value magnitude is 0.000001\n",
      "\tOn iter 9, average output value magnitude is 0.000001\n",
      "\tThe output from the final run was exactly this: \n",
      "\t[[[[-9.8479518e-09 -3.1693617e-09 -9.8479518e-09 -3.1693617e-09]\n",
      "\t   [-1.2038971e-08 -2.4224583e-08 -1.2038971e-08 -2.4224583e-08]\n",
      "\t   [-2.9213950e-08  3.5251446e-09 -2.9213950e-08  3.5251446e-09]\n",
      "\t   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n",
      "\t\n",
      "\t  [[ 7.8243954e-09 -1.3495761e-08  7.8243954e-09 -1.3495761e-08]\n",
      "\t   [ 1.1937953e-09 -1.9476968e-08  1.1937953e-09 -1.9476968e-08]\n",
      "\t   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "\t   [-1.0104617e-09 -9.2125285e-10 -1.0104617e-09 -9.2125285e-10]]\n",
      "\t\n",
      "\t  [[-1.6837802e-08  9.0102247e-08 -1.6837802e-08  9.0102247e-08]\n",
      "\t   [ 2.3615939e-08  3.8053258e-09  2.3615939e-08  3.8053258e-09]\n",
      "\t   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "\t   [ 2.9969407e-08  1.4605760e-08  2.9969407e-08  1.4605760e-08]]\n",
      "\t\n",
      "\t  [[-2.0265912e-08 -2.6968429e-08 -2.0265912e-08 -2.6968429e-08]\n",
      "\t   [ 2.5831035e-08  8.6098026e-09  2.5831035e-08  8.6098026e-09]\n",
      "\t   [ 1.9946071e-09  7.4294348e-10  1.9946071e-09  7.4294348e-10]\n",
      "\t   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test 3: same as test 1, with a small amount of noise added\n",
      "\tOn iter 0, average output value magnitude is 0.085789, ratio of magnitudes output/input is 0.005707\n",
      "\tOn iter 1, average output value magnitude is 0.115315, ratio of magnitudes output/input is 0.006573\n",
      "\tOn iter 2, average output value magnitude is 0.066266, ratio of magnitudes output/input is 0.004305\n",
      "\tOn iter 3, average output value magnitude is 0.151932, ratio of magnitudes output/input is 0.008761\n",
      "\tOn iter 4, average output value magnitude is 0.076960, ratio of magnitudes output/input is 0.005108\n",
      "\tOn iter 5, average output value magnitude is 0.133993, ratio of magnitudes output/input is 0.008283\n",
      "\tOn iter 6, average output value magnitude is 0.068280, ratio of magnitudes output/input is 0.005698\n",
      "\tOn iter 7, average output value magnitude is 0.112884, ratio of magnitudes output/input is 0.007398\n",
      "\tOn iter 8, average output value magnitude is 0.108362, ratio of magnitudes output/input is 0.006919\n",
      "\tOn iter 9, average output value magnitude is 0.080958, ratio of magnitudes output/input is 0.005044\n",
      "\tThe output from the final run was exactly this: \n",
      "\t[[[[ 8.9825591e-04 -1.1216545e-03  3.7734098e-05 -1.3503346e-03]\n",
      "\t   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "\t   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "\t   [ 1.0110158e-03 -1.3603122e-03  1.4404397e-04 -1.6627983e-03]]\n",
      "\t\n",
      "\t  [[ 2.7394688e-03 -3.4208356e-03  1.1508207e-04 -4.1182269e-03]\n",
      "\t   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "\t   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "\t   [-4.5909735e-04 -7.9808728e-04  1.4045923e-03 -1.3137473e-03]]\n",
      "\t\n",
      "\t  [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "\t   [-1.8232368e-03 -3.1693927e-03  5.5780825e-03 -5.2171266e-03]\n",
      "\t   [-1.5987337e-03 -2.7794796e-03  4.8915651e-03 -4.5751799e-03]\n",
      "\t   [ 1.9171738e-04 -1.9311517e-03  1.7646394e-03 -2.7603498e-03]]\n",
      "\t\n",
      "\t  [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "\t   [ 1.0539413e-03 -2.9005026e-03  1.6892849e-03 -3.8995722e-03]\n",
      "\t   [-7.2282628e-04 -1.2565104e-03  2.2114294e-03 -2.0683601e-03]\n",
      "\t   [ 1.6760002e-04 -2.0048090e-03  1.8713129e-03 -2.8756575e-03]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test 4: same as test 3, with batch norm\n",
      "\tOn iter 0, average output value magnitude is 0.058531, ratio of magnitudes output/input is 0.004017\n",
      "\tOn iter 1, average output value magnitude is 0.053668, ratio of magnitudes output/input is 0.004128\n",
      "\tOn iter 2, average output value magnitude is 0.053938, ratio of magnitudes output/input is 0.003421\n",
      "\tOn iter 3, average output value magnitude is 0.060155, ratio of magnitudes output/input is 0.005188\n",
      "\tOn iter 4, average output value magnitude is 0.056877, ratio of magnitudes output/input is 0.003461\n",
      "\tOn iter 5, average output value magnitude is 0.055579, ratio of magnitudes output/input is 0.003425\n",
      "\tOn iter 6, average output value magnitude is 0.057788, ratio of magnitudes output/input is 0.003789\n",
      "\tOn iter 7, average output value magnitude is 0.053744, ratio of magnitudes output/input is 0.003246\n",
      "\tOn iter 8, average output value magnitude is 0.055922, ratio of magnitudes output/input is 0.003703\n",
      "\tOn iter 9, average output value magnitude is 0.057725, ratio of magnitudes output/input is 0.003729\n",
      "\tThe output from the final run was exactly this: \n",
      "\t[[[[ 9.3192339e-04 -2.5731558e-03 -1.2845665e-03  1.6399239e-03]\n",
      "\t   [ 1.5345245e-04  4.0507164e-08  1.0753815e-03  3.7306341e-04]\n",
      "\t   [ 2.4477439e-04  6.5980764e-08  1.7153665e-03  5.9508043e-04]\n",
      "\t   [ 4.7279306e-04 -4.7289787e-04  1.8767326e-03  1.0344101e-03]]\n",
      "\t\n",
      "\t  [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "\t   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "\t   [ 6.1911014e-05  1.6430034e-08  4.3386786e-04  1.5051455e-04]\n",
      "\t   [ 8.7610877e-04 -2.4190363e-03 -1.2076508e-03  1.5416984e-03]]\n",
      "\t\n",
      "\t  [[ 1.1934885e-03 -1.7661850e-03  2.9989036e-03  2.4719953e-03]\n",
      "\t   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "\t   [ 3.3666563e-04 -9.2546310e-04 -4.5156878e-04  5.9344654e-04]\n",
      "\t   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n",
      "\t\n",
      "\t  [[ 1.1425395e-03 -8.4927602e-04  5.4264893e-03  2.5710489e-03]\n",
      "\t   [ 1.1690432e-03 -2.6189727e-03  2.3786267e-04  2.2052669e-03]\n",
      "\t   [ 9.1719633e-04 -1.7697037e-03  1.0521966e-03  1.7994752e-03]\n",
      "\t   [ 3.9224047e-04 -1.7893180e-07  2.7478347e-03  9.5349690e-04]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test 5: compare outputs, with the same input, given different noise ratios (no batch norm or max pool)\n",
      "\tNoise ratio of 1.000000E-10, \n",
      "\t\tgives average output magnitude of 1.666955E-06,\n",
      "\t\tratio of output/input magnitudes is 8.957741E-08\n",
      "\tNoise ratio of 1.000000E-09, \n",
      "\t\tgives average output magnitude of 4.219008E-06,\n",
      "\t\tratio of output/input magnitudes is 2.267175E-07\n",
      "\tNoise ratio of 1.000000E-08, \n",
      "\t\tgives average output magnitude of 1.549503E-06,\n",
      "\t\tratio of output/input magnitudes is 8.326588E-08\n",
      "\tNoise ratio of 1.000000E-07, \n",
      "\t\tgives average output magnitude of 9.731090E-06,\n",
      "\t\tratio of output/input magnitudes is 5.229211E-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNoise ratio of 1.000000E-06, \n",
      "\t\tgives average output magnitude of 2.119790E-04,\n",
      "\t\tratio of output/input magnitudes is 1.139115E-05\n",
      "\tNoise ratio of 1.000000E-05, \n",
      "\t\tgives average output magnitude of 1.297370E-03,\n",
      "\t\tratio of output/input magnitudes is 6.971697E-05\n",
      "\tNoise ratio of 1.000000E-04, \n",
      "\t\tgives average output magnitude of 3.038116E-02,\n",
      "\t\tratio of output/input magnitudes is 1.632597E-03\n",
      "\tNoise ratio of 1.000000E-03, \n",
      "\t\tgives average output magnitude of 9.491387E-02,\n",
      "\t\tratio of output/input magnitudes is 5.100401E-03\n",
      "\tNoise ratio of 1.000000E-02, \n",
      "\t\tgives average output magnitude of 1.074404E+00,\n",
      "\t\tratio of output/input magnitudes is 5.773542E-02\n",
      "\tNoise ratio of 1.000000E-01, \n",
      "\t\tgives average output magnitude of 1.294259E+01,\n",
      "\t\tratio of output/input magnitudes is 6.954979E-01\n",
      "\tNoise ratio of 1.000000E+00, \n",
      "\t\tgives average output magnitude of 8.682350E+01,\n",
      "\t\tratio of output/input magnitudes is 4.665648E+00\n",
      "\tNoise ratio of 1.000000E+01, \n",
      "\t\tgives average output magnitude of 9.722620E+02,\n",
      "\t\tratio of output/input magnitudes is 5.224659E+01\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test 6: same as test 5, with batch norm added\n",
      "\tNoise ratio of 1.000000E-10, \n",
      "\t\tgives average output magnitude of 1.306008E-06,\n",
      "\t\tratio of output/input magnitudes is 8.834010E-08\n",
      "\tNoise ratio of 1.000000E-09, \n",
      "\t\tgives average output magnitude of 2.634708E-06,\n",
      "\t\tratio of output/input magnitudes is 1.782151E-07\n",
      "\tNoise ratio of 1.000000E-08, \n",
      "\t\tgives average output magnitude of 3.615911E-07,\n",
      "\t\tratio of output/input magnitudes is 2.445850E-08\n",
      "\tNoise ratio of 1.000000E-07, \n",
      "\t\tgives average output magnitude of 8.340758E-06,\n",
      "\t\tratio of output/input magnitudes is 5.641799E-07\n",
      "\tNoise ratio of 1.000000E-06, \n",
      "\t\tgives average output magnitude of 1.598134E-04,\n",
      "\t\tratio of output/input magnitudes is 1.080999E-05\n",
      "\tNoise ratio of 1.000000E-05, \n",
      "\t\tgives average output magnitude of 1.480199E-03,\n",
      "\t\tratio of output/input magnitudes is 1.001226E-04\n",
      "\tNoise ratio of 1.000000E-04, \n",
      "\t\tgives average output magnitude of 1.133087E-02,\n",
      "\t\tratio of output/input magnitudes is 7.664354E-04\n",
      "\tNoise ratio of 1.000000E-03, \n",
      "\t\tgives average output magnitude of 2.159623E-01,\n",
      "\t\tratio of output/input magnitudes is 1.460798E-02\n",
      "\tNoise ratio of 1.000000E-02, \n",
      "\t\tgives average output magnitude of 1.726968E+00,\n",
      "\t\tratio of output/input magnitudes is 1.168144E-01\n",
      "\tNoise ratio of 1.000000E-01, \n",
      "\t\tgives average output magnitude of 9.587515E+00,\n",
      "\t\tratio of output/input magnitudes is 6.485122E-01\n",
      "\tNoise ratio of 1.000000E+00, \n",
      "\t\tgives average output magnitude of 8.459512E+01,\n",
      "\t\tratio of output/input magnitudes is 5.722126E+00\n",
      "\tNoise ratio of 1.000000E+01, \n",
      "\t\tgives average output magnitude of 1.554509E+03,\n",
      "\t\tratio of output/input magnitudes is 1.051491E+02\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Helper to make printing a little prettier :)\n",
    "def indprint(string, tabs=0):\n",
    "    print(\"\\t\" * tabs + string.replace('\\n', '\\n'+'\\t'*tabs))\n",
    "    \n",
    "##########\n",
    "# Test 1 #\n",
    "##########\n",
    "tf.reset_default_graph()\n",
    "indprint(\"Test 1: Zero initialized module, no batch norm, zero noise, check with 10 different inputs that the output is zero.\")\n",
    "# One layer network\n",
    "inputs_placeholder = tf.placeholder(shape=[None,4,4,2], dtype=tf.float32, name=\"input_placeholder\")\n",
    "output_op = init_zero_module_v1(inputs=inputs_placeholder, \n",
    "                                output_channels=4, \n",
    "                                add_max_pool=False, \n",
    "                                add_batch_norm=False, \n",
    "                                noise_ratio=0.0, \n",
    "                                scope=\"myscope\")\n",
    "output = None\n",
    "\n",
    "# tf init\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# run with 10 different random inputs\n",
    "for i in range(10):\n",
    "    rand_inputs = np.random.uniform(low=-1.0, high=1.0, size=(1,4,4,2))\n",
    "    feed_dict = {inputs_placeholder: rand_inputs}\n",
    "    output = sess.run([output_op], feed_dict=feed_dict)\n",
    "    indprint(\"On iter %d, average output value magnitude is %f\" % (i, np.sum(np.absolute(output[0][0]))), 1)\n",
    "indprint(\"The output from the final run was exactly this: \", 1)\n",
    "indprint(np.array2string(output[0][0]), 1)\n",
    "\n",
    "print('\\n' * 4)\n",
    "\n",
    "\n",
    "##########\n",
    "# Test 2 #\n",
    "##########\n",
    "tf.reset_default_graph()\n",
    "indprint(\"Test 2: same as test 1, with batch norm added, and max pooling added\")\n",
    "inputs_placeholder = tf.placeholder(shape=[None,4,4,2], dtype=tf.float32, name=\"input_placeholder\")\n",
    "output_op = init_zero_module_v1(inputs=inputs_placeholder, \n",
    "                                output_channels=4, \n",
    "                                add_max_pool=True, \n",
    "                                add_batch_norm=True, \n",
    "                                noise_ratio=0.0, \n",
    "                                scope=\"myscope2\")\n",
    "output = None\n",
    "\n",
    "# tf init\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# run with 10 different random inputs\n",
    "for i in range(10):\n",
    "    rand_inputs = np.random.uniform(low=-1.0, high=1.0, size=(1,4,4,2))\n",
    "    feed_dict = {inputs_placeholder: rand_inputs}\n",
    "    output = sess.run([output_op], feed_dict=feed_dict)\n",
    "    indprint(\"On iter %d, average output value magnitude is %f\" % (i, np.sum(np.absolute(output[0][0]))), 1)\n",
    "indprint(\"The output from the final run was exactly this: \", 1)\n",
    "indprint(np.array2string(output[0][0]), 1)\n",
    "\n",
    "print('\\n' * 4)\n",
    "\n",
    "\n",
    "##########\n",
    "# Test 3 #\n",
    "##########\n",
    "tf.reset_default_graph()\n",
    "indprint(\"Test 3: same as test 1, with a small amount of noise added\")\n",
    "inputs_placeholder = tf.placeholder(shape=[None,4,4,2], dtype=tf.float32, name=\"input_placeholder\")\n",
    "output_op = init_zero_module_v1(inputs=inputs_placeholder, \n",
    "                                output_channels=4, \n",
    "                                add_max_pool=False, \n",
    "                                add_batch_norm=False, \n",
    "                                noise_ratio=1e-3, \n",
    "                                scope=\"myscope3\")\n",
    "output = None\n",
    "\n",
    "# tf init\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# run with 10 different random inputs\n",
    "for i in range(10):\n",
    "    rand_inputs = np.random.uniform(low=-1.0, high=1.0, size=(1,4,4,2))\n",
    "    feed_dict = {inputs_placeholder: rand_inputs}\n",
    "    output = sess.run([output_op], feed_dict=feed_dict)\n",
    "    vals = (i, np.sum(np.absolute(output[0][0])), np.sum(np.absolute(output[0][0]))/np.sum(np.absolute(rand_inputs)))\n",
    "    indprint(\"On iter %d, average output value magnitude is %f, ratio of magnitudes output/input is %f\" \n",
    "             % vals, 1)\n",
    "indprint(\"The output from the final run was exactly this: \", 1)\n",
    "indprint(np.array2string(output[0][0]), 1)\n",
    "\n",
    "print('\\n' * 4)\n",
    "\n",
    "\n",
    "##########\n",
    "# Test 4 #\n",
    "##########\n",
    "tf.reset_default_graph()\n",
    "indprint(\"Test 4: same as test 3, with batch norm\")\n",
    "inputs_placeholder = tf.placeholder(shape=[None,4,4,2], dtype=tf.float32, name=\"input_placeholder\")\n",
    "output_op = init_zero_module_v1(inputs=inputs_placeholder, \n",
    "                                output_channels=4, \n",
    "                                add_max_pool=False, \n",
    "                                add_batch_norm=True, \n",
    "                                noise_ratio=1e-3, \n",
    "                                scope=\"myscope4\")\n",
    "output = None\n",
    "\n",
    "# tf init\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# run with 10 different random inputs\n",
    "for i in range(10):\n",
    "    rand_inputs = np.random.uniform(low=-1.0, high=1.0, size=(1,4,4,2))\n",
    "    feed_dict = {inputs_placeholder: rand_inputs}\n",
    "    output = sess.run([output_op], feed_dict=feed_dict)\n",
    "    vals = (i, np.sum(np.absolute(output[0][0])), np.sum(np.absolute(output[0][0]))/np.sum(np.absolute(rand_inputs)))\n",
    "    indprint(\"On iter %d, average output value magnitude is %f, ratio of magnitudes output/input is %f\" \n",
    "             % vals, 1)\n",
    "indprint(\"The output from the final run was exactly this: \", 1)\n",
    "indprint(np.array2string(output[0][0]), 1)\n",
    "\n",
    "print('\\n' * 4)\n",
    "\n",
    "\n",
    "##########\n",
    "# Test 5 #\n",
    "##########\n",
    "tf.reset_default_graph()\n",
    "indprint(\"Test 5: compare outputs, with the same input, given different noise ratios (no batch norm or max pool)\")\n",
    "inputs_placeholder = tf.placeholder(shape=[None,4,4,2], dtype=tf.float32, name=\"input_placeholder\")\n",
    "\n",
    "# fix some input\n",
    "rand_inputs = np.random.uniform(low=-1.0, high=1.0, size=(1,4,4,2))\n",
    "feed_dict = {inputs_placeholder: rand_inputs}\n",
    "\n",
    "# run with 10 different magnitudes of noise\n",
    "for noise in [10.0**i for i in range(-10, 2)]:\n",
    "    # Make the zero module, with the desired amount of noise\n",
    "    output_op = init_zero_module_v1(inputs=inputs_placeholder, \n",
    "                                    output_channels=4, \n",
    "                                    add_max_pool=False, \n",
    "                                    add_batch_norm=False, \n",
    "                                    noise_ratio=noise, \n",
    "                                    scope=\"myscope5\")\n",
    "    # tf init and run\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output = sess.run([output_op], feed_dict=feed_dict)\n",
    "    \n",
    "    # analysis\n",
    "    output_magnitudes = np.sum(np.absolute(output[0][0]))\n",
    "    output_magnitudes_ratio = np.sum(np.absolute(output[0][0]))/np.sum(np.absolute(rand_inputs))\n",
    "    vals = (noise, output_magnitudes, output_magnitudes_ratio) \n",
    "    indprint((\"Noise ratio of %E, \\n\\tgives average output magnitude of %E,\" + \n",
    "             \"\\n\\tratio of output/input magnitudes is %E\") % vals, 1)\n",
    "\n",
    "print('\\n' * 4)\n",
    "\n",
    "\n",
    "##########\n",
    "# Test 6 #\n",
    "##########\n",
    "tf.reset_default_graph()\n",
    "indprint(\"Test 6: same as test 5, with batch norm added\")\n",
    "inputs_placeholder = tf.placeholder(shape=[None,4,4,2], dtype=tf.float32, name=\"input_placeholder\")\n",
    "\n",
    "# fix some input\n",
    "rand_inputs = np.random.uniform(low=-1.0, high=1.0, size=(1,4,4,2))\n",
    "feed_dict = {inputs_placeholder: rand_inputs}\n",
    "\n",
    "# run with 10 different magnitudes of noise\n",
    "for noise in [10.0**i for i in range(-10, 2)]:\n",
    "    # Make the zero module, with the desired amount of noise\n",
    "    output_op = init_zero_module_v1(inputs=inputs_placeholder, \n",
    "                                    output_channels=4, \n",
    "                                    add_max_pool=False, \n",
    "                                    add_batch_norm=False, \n",
    "                                    noise_ratio=noise, \n",
    "                                    scope=\"myscope6\")\n",
    "    # tf init and run\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output = sess.run([output_op], feed_dict=feed_dict)\n",
    "    \n",
    "    # analysis\n",
    "    output_magnitudes = np.sum(np.absolute(output[0][0]))\n",
    "    output_magnitudes_ratio = np.sum(np.absolute(output[0][0]))/np.sum(np.absolute(rand_inputs))\n",
    "    vals = (noise, output_magnitudes, output_magnitudes_ratio) \n",
    "    indprint((\"Noise ratio of %E, \\n\\tgives average output magnitude of %E,\" + \n",
    "             \"\\n\\tratio of output/input magnitudes is %E\") % vals, 1)\n",
    "\n",
    "print('\\n' * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding residual connections and using zero initializations to initialize layers to identity transforms\n",
    "\n",
    "When we add residual connections over layers. When we have residual connections, our neural network learns a function $F(x)$, and the output is $h(x) = x + F(x)$. If we initialize $F(x)$ such that for all $x$ we have $F(x) = 0$, then we have $h(x)=x$ for any $x$, and therefore $h$ is the identity transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_resnet_module_v1(inputs, output_channels, add_batch_norm=True, noise_ratio=1e-3, scope=\"\"):\n",
    "    \"\"\"\n",
    "    Uses init_zero_module_v1 to initialize a neural network \n",
    "    \n",
    "    As this is a v1, we will zero initialize a 2 layer network, and then add a residual connection over it.\n",
    "    To do this, we do the following:\n",
    "    1. initialize an arbitrary layer\n",
    "    2. initialize a zero initialized convolution ontop of the first (this is F(x))\n",
    "    3. add the input to the output, to add the residual connection (to get x + F(x))\n",
    "    \n",
    "    Again, as this is a prototype, we add some arbitrary choices. For example, the number of channels from \n",
    "    each layer are [input_channels, output_channels, output_channels]\n",
    "    \n",
    "    Also no max pooling\n",
    "    \n",
    "    :param inputs: input to the zero module\n",
    "    :param num_output_filters: the number of output channels to have in the output\n",
    "    :param add_max_pool: should we add a max pool before anything else?\n",
    "    :param add_batch_norm: should we add a batch norm layer before the activation?\n",
    "    :param noise_ratio: the ratio (with respect to the max value in the filter) of noise to add\n",
    "    :param scope: tensorflow variable scoping\n",
    "    :return: output from the zero module\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        # nn layer 1, he initialized\n",
    "        initer = tf.contrib.layers.variance_scaling_initializer(factor=2.0)\n",
    "        linear = tf.layers.conv2d(inputs, output_channels, [4,4], padding=\"SAME\", kernel_initializer=initer, name=\"resnet_layer1\")\n",
    "        batch_norm = linear\n",
    "        if add_batch_norm: \n",
    "            batch_norm = tf.contrib.layers.batch_norm(linear)\n",
    "        relu_out = tf.nn.relu(batch_norm)\n",
    "        \n",
    "        # nn layer 2, zero initialized\n",
    "        network_out, _ = init_zero_module_v1(relu_out, output_channels, add_max_pool=False, \n",
    "                                             add_batch_norm=add_batch_norm, noise_ratio=noise_ratio, \n",
    "                                             scope=\"resnet_layer2\")\n",
    "        \n",
    "        # residual connection (pad inputs with zeros to match output shape)\n",
    "        resnet_out = network_out \n",
    "        in_channels = inputs.get_shape().as_list()[3] # inputs.shape = [batchsize, width, height, in_channels]\n",
    "        paddings = ((0,0), (0,0), (0,0), (0,max(0,output_channels - in_channels)))\n",
    "        output = resnet_out + tf.pad(inputs, paddings, 'CONSTANT')\n",
    "        \n",
    "        # done :)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More sanity checks\n",
    "\n",
    "So now we just do one more sanity check, which involves checking if we did indeed define an itentity operator above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 7: Check that the identity resnet module is indeed an identity function.\n",
      "\tOn iter 0, average difference between input and output is 0.000001\n",
      "\tOn iter 1, average difference between input and output is 0.000000\n",
      "\tOn iter 2, average difference between input and output is 0.000000\n",
      "\tOn iter 3, average difference between input and output is 0.000000\n",
      "\tOn iter 4, average difference between input and output is 0.000000\n",
      "\tOn iter 5, average difference between input and output is 0.000000\n",
      "\tOn iter 6, average difference between input and output is 0.000000\n",
      "\tOn iter 7, average difference between input and output is 0.000000\n",
      "\tOn iter 8, average difference between input and output is 0.000001\n",
      "\tOn iter 9, average difference between input and output is 0.000000\n",
      "\tThe output from the final run was exactly this: \n",
      "\t[[[-5.1089621e-01  7.8303033e-01 -3.3106957e-09  1.3657569e-08\n",
      "\t    2.5767349e-08 -3.3106957e-09]\n",
      "\t  [-7.4016052e-01 -3.5873368e-01  3.2226595e-09  3.2490099e-10\n",
      "\t    4.7995163e-10  3.2226595e-09]\n",
      "\t  [-8.7268460e-01 -4.5210844e-01  1.1332503e-08 -5.4377125e-09\n",
      "\t    2.6947333e-10  1.1332503e-08]\n",
      "\t  [ 6.3164961e-01 -4.9849603e-01 -7.3215443e-09  4.0354919e-09\n",
      "\t   -3.5776111e-09 -7.3215443e-09]]\n",
      "\t\n",
      "\t [[ 3.3246565e-01  8.6722869e-01 -1.2486012e-10  1.3618651e-08\n",
      "\t    2.8168659e-09 -1.2486012e-10]\n",
      "\t  [ 5.2869730e-02  8.6048132e-01 -2.0216920e-08 -1.4349624e-08\n",
      "\t   -6.0988441e-09 -2.0216920e-08]\n",
      "\t  [-3.0264452e-01 -6.1181915e-01 -5.1149968e-08  6.9878192e-09\n",
      "\t    3.8852903e-09 -5.1149968e-08]\n",
      "\t  [-3.3185521e-01  9.3878078e-01 -8.2568663e-10  2.0581421e-09\n",
      "\t   -9.5115809e-09 -8.2568663e-10]]\n",
      "\t\n",
      "\t [[-9.5925945e-01  2.3814444e-01  0.0000000e+00  0.0000000e+00\n",
      "\t    0.0000000e+00  0.0000000e+00]\n",
      "\t  [ 5.8576989e-01  8.2013786e-01 -4.8806648e-09  3.4492444e-09\n",
      "\t   -3.9601900e-08 -4.8806648e-09]\n",
      "\t  [-5.3746092e-01 -4.8894095e-01 -1.0572542e-08 -2.1330180e-08\n",
      "\t   -3.2102889e-08 -1.0572542e-08]\n",
      "\t  [-1.9197126e-01 -7.3213565e-01 -1.2132691e-08 -1.8499211e-09\n",
      "\t   -1.8716362e-09 -1.2132691e-08]]\n",
      "\t\n",
      "\t [[-4.8733331e-02  3.0080003e-01  1.2266068e-09 -2.0969360e-10\n",
      "\t    1.3775974e-08  1.2266068e-09]\n",
      "\t  [-6.3851845e-01  3.9420187e-01  0.0000000e+00  0.0000000e+00\n",
      "\t    0.0000000e+00  0.0000000e+00]\n",
      "\t  [-9.4984031e-01  8.5289991e-01  2.6171665e-10  9.0898578e-10\n",
      "\t    3.0793359e-09  2.6171665e-10]\n",
      "\t  [ 3.8829800e-01  5.4062241e-01  0.0000000e+00  0.0000000e+00\n",
      "\t    0.0000000e+00  0.0000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# Test 7 #\n",
    "##########\n",
    "tf.reset_default_graph()\n",
    "indprint(\"Test 7: Check that the identity resnet module is indeed an identity function.\")\n",
    "# One layer network\n",
    "inputs_placeholder = tf.placeholder(shape=[None,4,4,2], dtype=tf.float32, name=\"input_placeholder\")\n",
    "output_op = identity_resnet_module_v1(inputs=inputs_placeholder, \n",
    "                                      output_channels=6, \n",
    "                                      add_batch_norm=True, \n",
    "                                      noise_ratio=0.0, \n",
    "                                      scope=\"myyscope\")\n",
    "output = None\n",
    "\n",
    "# tf init\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# run with 10 different random inputs\n",
    "for i in range(10):\n",
    "    rand_inputs = np.random.uniform(low=-1.0, high=1.0, size=(1,4,4,2))\n",
    "    feed_dict = {inputs_placeholder: rand_inputs}\n",
    "    output = sess.run([output_op], feed_dict=feed_dict)\n",
    "    indprint(\"On iter %d, average difference between input and output is %f\" \n",
    "             % (i, np.sum(np.absolute(output[0][:,:,:,:2] - rand_inputs))), 1)\n",
    "indprint(\"The output from the final run was exactly this: \", 1)\n",
    "indprint(np.array2string(output[0][0]), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a small netowork, with 3 resnet modules, 3 max pools, and a fc output\n",
    "\n",
    "We constuct a small network, and check that it can learn mnist and cifar-10\n",
    "\n",
    "The first cell below defines a training loop used to evaluate our models, the second and third cells define functions that make networks. The fourth cell uses this to train and evaluate the models a number of times. The fifth cell will print out some graphs displaying the results. \n",
    "\n",
    "The remaining cells in this section run the same experiment with the CIFAR-10 dataset.\n",
    "\n",
    "TODO: cite 231n and sat how to get the dataset, and what command to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a model\n",
    "def evaluate(scores_pred_op, inputs_placeholder, labels_placeholder, dataset):\n",
    "    # Make an op to compute the current accuracy over a minibatch\n",
    "    correct_preds = tf.equal(tf.argmax(scores,axis=1), tf.argmax(labels_placeholder,axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n",
    "    \n",
    "    # construct optimizer\n",
    "    ce_loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_placeholder, logits=scores, name=\"celoss\")\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(ce_loss)\n",
    "\n",
    "    # training loop (saving accuracies over time)\n",
    "    train_iters = 300\n",
    "    epoch_len = 10\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(train_iters+1):\n",
    "        xs, ys = dataset.next_batch(64)\n",
    "        feed_dict = {inputs_placeholder: xs, labels_placeholder: ys}\n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "\n",
    "        if i % epoch_len == 0:\n",
    "            train_acc = sess.run(accuracy, feed_dict=feed_dict)\n",
    "\n",
    "            test_xs, test_ys = dataset.next_test_batch(128)\n",
    "            test_feed_dict = {inputs_placeholder: test_xs, labels_placeholder: test_ys}\n",
    "            test_acc = sess.run(accuracy, feed_dict=test_feed_dict)\n",
    "\n",
    "            train_accuracies.append(train_acc)\n",
    "            test_accuracies.append(test_acc)\n",
    "            \n",
    "            print(\"(Iter %d). Current train: %0.6f, Current test: %0.6f.\" % (i, train_acc, test_acc))\n",
    "    \n",
    "    return train_accuracies, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct identity initialized prediction network (small)\n",
    "def make_identity_init_mnist_network(inputs_placeholder, labels_placeholder):\n",
    "    initer = tf.contrib.layers.variance_scaling_initializer(factor=2.0)\n",
    "    \n",
    "    blockone = identity_resnet_module_v1(inputs_placeholder, 16, noise_ratio=0.0, scope=\"layerone\") # [64,32,32,16]\n",
    "    blockone = tf.layers.max_pooling2d(blockone, pool_size=2, strides=2) # [64,16,16,16]\n",
    "    \n",
    "    blocktwo = identity_resnet_module_v1(blockone, 32, noise_ratio=0.0, scope=\"layertwo\") # [64,16,16,32]\n",
    "    blocktwo = tf.layers.max_pooling2d(blocktwo, pool_size=2, strides=2) # [64,8,8,32]\n",
    "    \n",
    "    blockthree = identity_resnet_module_v1(blocktwo, 64, noise_ratio=0.0, scope=\"layerthree\") # [64,8,8,64]\n",
    "    blockthree = tf.layers.max_pooling2d(blockthree, pool_size=2, strides=2) # [64,4,4,64]\n",
    "    \n",
    "    fc_input = tf.reshape(blockthree, (-1, 4*4*64))\n",
    "    fc_hidden = tf.contrib.layers.fully_connected(fc_input, 256, weights_initializer=initer)\n",
    "    scores = tf.contrib.layers.fully_connected(fc_hidden, 10, weights_initializer=initer, activation_fn=tf.identity)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a he initialized prediction network (small)\n",
    "def make_he_init_mnist_network(inputs_placeholder, labels_placeholder):\n",
    "    # He initializer\n",
    "    initer = tf.contrib.layers.variance_scaling_initializer(factor=2.0)\n",
    "        \n",
    "    def res_block(inputs, channels):\n",
    "        # conv layers (he initialized)\n",
    "        outputs = tf.layers.conv2d(inputs, channels, [4,4], padding=\"SAME\", kernel_initializer=initer)\n",
    "        outputs = tf.contrib.layers.batch_norm(outputs)\n",
    "        outputs = tf.nn.relu(outputs)\n",
    "        outputs = tf.layers.conv2d(outputs, channels, [4,4], padding=\"SAME\", kernel_initializer=initer)\n",
    "        outputs = tf.contrib.layers.batch_norm(outputs)\n",
    "        outputs = tf.nn.relu(outputs)\n",
    "        \n",
    "        # residual connection (pad inputs with zeros to match output shape)\n",
    "        in_channels = inputs.get_shape().as_list()[3] # inputs.shape = [batchsize, width, height, in_channels]\n",
    "        paddings = ((0,0), (0,0), (0,0), (0,max(0,channels - in_channels)))\n",
    "        outputs = outputs + tf.pad(inputs, paddings, 'CONSTANT')\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    # Actually construct network\n",
    "    blockone = res_block(inputs_placeholder, 16) # [64,32,32,16]\n",
    "    blockone = tf.layers.max_pooling2d(blockone, pool_size=2, strides=2) # [64,16,16,16]\n",
    "    \n",
    "    blocktwo = res_block(blockone, 32) # [64,16,16,32]\n",
    "    blocktwo = tf.layers.max_pooling2d(blocktwo, pool_size=2, strides=2) # [64,8,8,32]\n",
    "    \n",
    "    blockthree = res_block(blocktwo, 64) # [64,8,8,64]\n",
    "    blockthree = tf.layers.max_pooling2d(blockthree, pool_size=2, strides=2) # [64,4,4,64]\n",
    "    \n",
    "    fc_input = tf.reshape(blockthree, (-1, 4*4*64))\n",
    "    fc_hidden = tf.contrib.layers.fully_connected(fc_input, 256, weights_initializer=initer)\n",
    "    scores = tf.contrib.layers.fully_connected(fc_hidden, 10, weights_initializer=initer, activation_fn=tf.identity)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mp/231n/Deep-Neuroevolution-With-SharedWeights---Ensembling-A-Better-Solution/dataset/dataset_mnist.py:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /anaconda/envs/cs231n/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /anaconda/envs/cs231n/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:219: retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /anaconda/envs/cs231n/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/mp/231n/Deep-Neuroevolution-With-SharedWeights---Ensembling-A-Better-Solution/dataset/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /anaconda/envs/cs231n/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/mp/231n/Deep-Neuroevolution-With-SharedWeights---Ensembling-A-Better-Solution/dataset/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda/envs/cs231n/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /home/mp/231n/Deep-Neuroevolution-With-SharedWeights---Ensembling-A-Better-Solution/dataset/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /home/mp/231n/Deep-Neuroevolution-With-SharedWeights---Ensembling-A-Better-Solution/dataset/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda/envs/cs231n/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-10-43eae45a713f>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "(Iter 0). Current train: 0.125000, Current test: 0.132812.\n",
      "(Iter 10). Current train: 0.390625, Current test: 0.375000.\n",
      "(Iter 20). Current train: 0.562500, Current test: 0.593750.\n",
      "(Iter 30). Current train: 0.578125, Current test: 0.617188.\n",
      "(Iter 40). Current train: 0.718750, Current test: 0.640625.\n",
      "(Iter 50). Current train: 0.609375, Current test: 0.710938.\n",
      "(Iter 60). Current train: 0.718750, Current test: 0.812500.\n",
      "(Iter 70). Current train: 0.781250, Current test: 0.789062.\n",
      "(Iter 80). Current train: 0.812500, Current test: 0.851562.\n",
      "(Iter 90). Current train: 0.843750, Current test: 0.820312.\n",
      "(Iter 100). Current train: 0.921875, Current test: 0.859375.\n",
      "(Iter 110). Current train: 0.796875, Current test: 0.843750.\n",
      "(Iter 120). Current train: 0.953125, Current test: 0.906250.\n",
      "(Iter 130). Current train: 0.859375, Current test: 0.867188.\n",
      "(Iter 140). Current train: 0.968750, Current test: 0.914062.\n",
      "(Iter 150). Current train: 0.921875, Current test: 0.929688.\n",
      "(Iter 160). Current train: 0.890625, Current test: 0.906250.\n",
      "(Iter 170). Current train: 0.921875, Current test: 0.945312.\n",
      "(Iter 180). Current train: 0.921875, Current test: 0.914062.\n",
      "(Iter 190). Current train: 0.921875, Current test: 0.906250.\n",
      "(Iter 200). Current train: 0.906250, Current test: 0.875000.\n",
      "(Iter 210). Current train: 0.953125, Current test: 0.898438.\n",
      "(Iter 220). Current train: 0.968750, Current test: 0.976562.\n",
      "(Iter 230). Current train: 0.968750, Current test: 0.945312.\n",
      "(Iter 240). Current train: 0.953125, Current test: 0.914062.\n",
      "(Iter 250). Current train: 0.968750, Current test: 0.953125.\n",
      "(Iter 260). Current train: 0.937500, Current test: 0.929688.\n",
      "(Iter 270). Current train: 1.000000, Current test: 0.960938.\n",
      "(Iter 280). Current train: 0.953125, Current test: 0.953125.\n",
      "(Iter 290). Current train: 0.953125, Current test: 0.921875.\n",
      "(Iter 300). Current train: 0.937500, Current test: 0.945312.\n",
      "(Iter 0). Current train: 0.156250, Current test: 0.179688.\n",
      "(Iter 10). Current train: 0.343750, Current test: 0.281250.\n",
      "(Iter 20). Current train: 0.515625, Current test: 0.500000.\n",
      "(Iter 30). Current train: 0.687500, Current test: 0.593750.\n",
      "(Iter 40). Current train: 0.593750, Current test: 0.750000.\n",
      "(Iter 50). Current train: 0.734375, Current test: 0.687500.\n",
      "(Iter 60). Current train: 0.843750, Current test: 0.757812.\n",
      "(Iter 70). Current train: 0.765625, Current test: 0.789062.\n",
      "(Iter 80). Current train: 0.843750, Current test: 0.843750.\n",
      "(Iter 90). Current train: 0.875000, Current test: 0.828125.\n",
      "(Iter 100). Current train: 0.843750, Current test: 0.867188.\n",
      "(Iter 110). Current train: 0.953125, Current test: 0.875000.\n",
      "(Iter 120). Current train: 0.937500, Current test: 0.921875.\n",
      "(Iter 130). Current train: 0.875000, Current test: 0.906250.\n",
      "(Iter 140). Current train: 0.890625, Current test: 0.851562.\n",
      "(Iter 150). Current train: 0.921875, Current test: 0.953125.\n",
      "(Iter 160). Current train: 0.890625, Current test: 0.914062.\n",
      "(Iter 170). Current train: 0.921875, Current test: 0.906250.\n",
      "(Iter 180). Current train: 0.859375, Current test: 0.890625.\n",
      "(Iter 190). Current train: 0.921875, Current test: 0.898438.\n",
      "(Iter 200). Current train: 0.953125, Current test: 0.921875.\n",
      "(Iter 210). Current train: 0.937500, Current test: 0.929688.\n",
      "(Iter 220). Current train: 0.921875, Current test: 0.937500.\n",
      "(Iter 230). Current train: 0.890625, Current test: 0.906250.\n",
      "(Iter 240). Current train: 0.984375, Current test: 0.914062.\n",
      "(Iter 250). Current train: 0.984375, Current test: 0.960938.\n",
      "(Iter 260). Current train: 0.906250, Current test: 0.906250.\n",
      "(Iter 270). Current train: 0.875000, Current test: 0.945312.\n",
      "(Iter 280). Current train: 0.953125, Current test: 0.945312.\n",
      "(Iter 290). Current train: 0.937500, Current test: 0.929688.\n",
      "(Iter 300). Current train: 0.953125, Current test: 0.945312.\n",
      "(Iter 0). Current train: 0.156250, Current test: 0.085938.\n",
      "(Iter 10). Current train: 0.375000, Current test: 0.437500.\n",
      "(Iter 20). Current train: 0.593750, Current test: 0.640625.\n",
      "(Iter 30). Current train: 0.531250, Current test: 0.617188.\n",
      "(Iter 40). Current train: 0.750000, Current test: 0.757812.\n",
      "(Iter 50). Current train: 0.765625, Current test: 0.742188.\n",
      "(Iter 60). Current train: 0.812500, Current test: 0.781250.\n",
      "(Iter 70). Current train: 0.750000, Current test: 0.789062.\n",
      "(Iter 80). Current train: 0.828125, Current test: 0.859375.\n",
      "(Iter 90). Current train: 0.890625, Current test: 0.875000.\n",
      "(Iter 100). Current train: 0.828125, Current test: 0.921875.\n",
      "(Iter 110). Current train: 0.781250, Current test: 0.875000.\n",
      "(Iter 120). Current train: 0.937500, Current test: 0.914062.\n",
      "(Iter 130). Current train: 0.812500, Current test: 0.890625.\n",
      "(Iter 140). Current train: 0.859375, Current test: 0.921875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iter 150). Current train: 0.906250, Current test: 0.898438.\n",
      "(Iter 160). Current train: 0.906250, Current test: 0.937500.\n",
      "(Iter 170). Current train: 0.984375, Current test: 0.937500.\n",
      "(Iter 180). Current train: 0.921875, Current test: 0.921875.\n",
      "(Iter 190). Current train: 0.937500, Current test: 0.953125.\n",
      "(Iter 200). Current train: 0.906250, Current test: 0.914062.\n",
      "(Iter 210). Current train: 0.906250, Current test: 0.929688.\n",
      "(Iter 220). Current train: 0.937500, Current test: 0.960938.\n",
      "(Iter 230). Current train: 0.984375, Current test: 0.953125.\n",
      "(Iter 240). Current train: 0.921875, Current test: 0.929688.\n",
      "(Iter 250). Current train: 0.937500, Current test: 0.953125.\n",
      "(Iter 260). Current train: 0.968750, Current test: 0.945312.\n",
      "(Iter 270). Current train: 0.953125, Current test: 0.968750.\n",
      "(Iter 280). Current train: 0.984375, Current test: 0.953125.\n",
      "(Iter 290). Current train: 0.968750, Current test: 0.968750.\n",
      "(Iter 300). Current train: 0.968750, Current test: 0.937500.\n",
      "(Iter 0). Current train: 0.109375, Current test: 0.062500.\n",
      "(Iter 10). Current train: 0.406250, Current test: 0.265625.\n",
      "(Iter 20). Current train: 0.484375, Current test: 0.515625.\n",
      "(Iter 30). Current train: 0.593750, Current test: 0.648438.\n",
      "(Iter 40). Current train: 0.671875, Current test: 0.781250.\n",
      "(Iter 50). Current train: 0.687500, Current test: 0.750000.\n",
      "(Iter 60). Current train: 0.828125, Current test: 0.781250.\n",
      "(Iter 70). Current train: 0.750000, Current test: 0.820312.\n",
      "(Iter 80). Current train: 0.875000, Current test: 0.812500.\n",
      "(Iter 90). Current train: 0.875000, Current test: 0.867188.\n",
      "(Iter 100). Current train: 0.812500, Current test: 0.859375.\n",
      "(Iter 110). Current train: 0.875000, Current test: 0.890625.\n",
      "(Iter 120). Current train: 0.921875, Current test: 0.906250.\n",
      "(Iter 130). Current train: 0.953125, Current test: 0.906250.\n",
      "(Iter 140). Current train: 0.953125, Current test: 0.867188.\n",
      "(Iter 150). Current train: 0.875000, Current test: 0.867188.\n",
      "(Iter 160). Current train: 0.781250, Current test: 0.921875.\n",
      "(Iter 170). Current train: 0.906250, Current test: 0.929688.\n",
      "(Iter 180). Current train: 0.890625, Current test: 0.945312.\n",
      "(Iter 190). Current train: 0.937500, Current test: 0.929688.\n",
      "(Iter 200). Current train: 0.890625, Current test: 0.929688.\n",
      "(Iter 210). Current train: 0.937500, Current test: 0.945312.\n",
      "(Iter 220). Current train: 0.953125, Current test: 0.929688.\n",
      "(Iter 230). Current train: 0.921875, Current test: 0.914062.\n",
      "(Iter 240). Current train: 0.984375, Current test: 0.960938.\n",
      "(Iter 250). Current train: 0.984375, Current test: 0.945312.\n",
      "(Iter 260). Current train: 0.953125, Current test: 0.953125.\n",
      "(Iter 270). Current train: 0.968750, Current test: 0.968750.\n",
      "(Iter 280). Current train: 0.937500, Current test: 0.929688.\n",
      "(Iter 290). Current train: 0.921875, Current test: 0.937500.\n",
      "(Iter 300). Current train: 0.937500, Current test: 0.937500.\n",
      "(Iter 0). Current train: 0.109375, Current test: 0.101562.\n",
      "(Iter 10). Current train: 0.406250, Current test: 0.343750.\n",
      "(Iter 20). Current train: 0.546875, Current test: 0.507812.\n",
      "(Iter 30). Current train: 0.562500, Current test: 0.617188.\n",
      "(Iter 40). Current train: 0.656250, Current test: 0.617188.\n",
      "(Iter 50). Current train: 0.781250, Current test: 0.703125.\n",
      "(Iter 60). Current train: 0.875000, Current test: 0.789062.\n",
      "(Iter 70). Current train: 0.859375, Current test: 0.789062.\n",
      "(Iter 80). Current train: 0.812500, Current test: 0.882812.\n",
      "(Iter 90). Current train: 0.890625, Current test: 0.859375.\n",
      "(Iter 100). Current train: 0.843750, Current test: 0.875000.\n",
      "(Iter 110). Current train: 0.890625, Current test: 0.875000.\n",
      "(Iter 120). Current train: 0.890625, Current test: 0.906250.\n",
      "(Iter 130). Current train: 0.921875, Current test: 0.929688.\n",
      "(Iter 140). Current train: 0.843750, Current test: 0.859375.\n",
      "(Iter 150). Current train: 0.859375, Current test: 0.914062.\n",
      "(Iter 160). Current train: 0.796875, Current test: 0.898438.\n",
      "(Iter 170). Current train: 0.843750, Current test: 0.914062.\n",
      "(Iter 180). Current train: 0.937500, Current test: 0.945312.\n",
      "(Iter 190). Current train: 0.937500, Current test: 0.875000.\n",
      "(Iter 200). Current train: 0.968750, Current test: 0.960938.\n",
      "(Iter 210). Current train: 0.921875, Current test: 0.906250.\n",
      "(Iter 220). Current train: 0.953125, Current test: 0.953125.\n",
      "(Iter 230). Current train: 0.921875, Current test: 0.945312.\n",
      "(Iter 240). Current train: 0.953125, Current test: 0.929688.\n",
      "(Iter 250). Current train: 0.968750, Current test: 0.945312.\n",
      "(Iter 260). Current train: 0.937500, Current test: 0.929688.\n",
      "(Iter 270). Current train: 0.968750, Current test: 0.976562.\n",
      "(Iter 280). Current train: 0.937500, Current test: 0.882812.\n",
      "(Iter 290). Current train: 0.953125, Current test: 0.968750.\n",
      "(Iter 300). Current train: 0.921875, Current test: 0.968750.\n",
      "(Iter 0). Current train: 0.125000, Current test: 0.078125.\n",
      "(Iter 10). Current train: 0.250000, Current test: 0.265625.\n",
      "(Iter 20). Current train: 0.437500, Current test: 0.492188.\n",
      "(Iter 30). Current train: 0.609375, Current test: 0.593750.\n",
      "(Iter 40). Current train: 0.671875, Current test: 0.742188.\n",
      "(Iter 50). Current train: 0.734375, Current test: 0.695312.\n",
      "(Iter 60). Current train: 0.828125, Current test: 0.812500.\n",
      "(Iter 70). Current train: 0.859375, Current test: 0.843750.\n",
      "(Iter 80). Current train: 0.781250, Current test: 0.843750.\n",
      "(Iter 90). Current train: 0.890625, Current test: 0.882812.\n",
      "(Iter 100). Current train: 0.828125, Current test: 0.929688.\n",
      "(Iter 110). Current train: 0.953125, Current test: 0.867188.\n",
      "(Iter 120). Current train: 0.968750, Current test: 0.906250.\n",
      "(Iter 130). Current train: 0.890625, Current test: 0.960938.\n",
      "(Iter 140). Current train: 0.921875, Current test: 0.929688.\n",
      "(Iter 150). Current train: 0.906250, Current test: 0.898438.\n",
      "(Iter 160). Current train: 0.921875, Current test: 0.937500.\n",
      "(Iter 170). Current train: 0.937500, Current test: 0.937500.\n",
      "(Iter 180). Current train: 0.890625, Current test: 0.914062.\n",
      "(Iter 190). Current train: 0.921875, Current test: 0.867188.\n",
      "(Iter 200). Current train: 0.921875, Current test: 0.937500.\n",
      "(Iter 210). Current train: 0.953125, Current test: 0.945312.\n",
      "(Iter 220). Current train: 0.906250, Current test: 0.914062.\n",
      "(Iter 230). Current train: 0.906250, Current test: 0.960938.\n",
      "(Iter 240). Current train: 0.937500, Current test: 0.937500.\n",
      "(Iter 250). Current train: 0.953125, Current test: 0.937500.\n",
      "(Iter 260). Current train: 0.953125, Current test: 0.945312.\n",
      "(Iter 270). Current train: 0.921875, Current test: 0.953125.\n",
      "(Iter 280). Current train: 0.968750, Current test: 0.937500.\n",
      "(Iter 290). Current train: 0.984375, Current test: 0.945312.\n",
      "(Iter 300). Current train: 0.984375, Current test: 0.929688.\n",
      "(Iter 0). Current train: 0.093750, Current test: 0.140625.\n",
      "(Iter 10). Current train: 0.187500, Current test: 0.210938.\n",
      "(Iter 20). Current train: 0.468750, Current test: 0.359375.\n",
      "(Iter 30). Current train: 0.640625, Current test: 0.578125.\n",
      "(Iter 40). Current train: 0.625000, Current test: 0.710938.\n",
      "(Iter 50). Current train: 0.703125, Current test: 0.726562.\n",
      "(Iter 60). Current train: 0.828125, Current test: 0.695312.\n",
      "(Iter 70). Current train: 0.781250, Current test: 0.789062.\n",
      "(Iter 80). Current train: 0.828125, Current test: 0.843750.\n",
      "(Iter 90). Current train: 0.812500, Current test: 0.867188.\n",
      "(Iter 100). Current train: 0.843750, Current test: 0.812500.\n",
      "(Iter 110). Current train: 0.890625, Current test: 0.875000.\n",
      "(Iter 120). Current train: 0.890625, Current test: 0.882812.\n",
      "(Iter 130). Current train: 0.906250, Current test: 0.843750.\n",
      "(Iter 140). Current train: 0.890625, Current test: 0.906250.\n",
      "(Iter 150). Current train: 0.968750, Current test: 0.867188.\n",
      "(Iter 160). Current train: 0.906250, Current test: 0.937500.\n",
      "(Iter 170). Current train: 0.937500, Current test: 0.843750.\n",
      "(Iter 180). Current train: 0.937500, Current test: 0.921875.\n",
      "(Iter 190). Current train: 0.937500, Current test: 0.929688.\n",
      "(Iter 200). Current train: 0.937500, Current test: 0.929688.\n",
      "(Iter 210). Current train: 0.968750, Current test: 0.953125.\n",
      "(Iter 220). Current train: 0.921875, Current test: 0.921875.\n",
      "(Iter 230). Current train: 0.984375, Current test: 0.937500.\n",
      "(Iter 240). Current train: 0.968750, Current test: 0.882812.\n",
      "(Iter 250). Current train: 0.968750, Current test: 0.945312.\n",
      "(Iter 260). Current train: 0.921875, Current test: 0.929688.\n",
      "(Iter 270). Current train: 0.890625, Current test: 0.929688.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iter 280). Current train: 0.921875, Current test: 0.968750.\n",
      "(Iter 290). Current train: 0.937500, Current test: 0.937500.\n",
      "(Iter 300). Current train: 0.906250, Current test: 0.937500.\n",
      "(Iter 0). Current train: 0.015625, Current test: 0.007812.\n",
      "(Iter 10). Current train: 0.203125, Current test: 0.234375.\n",
      "(Iter 20). Current train: 0.390625, Current test: 0.500000.\n",
      "(Iter 30). Current train: 0.578125, Current test: 0.554688.\n",
      "(Iter 40). Current train: 0.687500, Current test: 0.648438.\n",
      "(Iter 50). Current train: 0.781250, Current test: 0.765625.\n",
      "(Iter 60). Current train: 0.812500, Current test: 0.804688.\n",
      "(Iter 70). Current train: 0.859375, Current test: 0.828125.\n",
      "(Iter 80). Current train: 0.828125, Current test: 0.921875.\n",
      "(Iter 90). Current train: 0.921875, Current test: 0.898438.\n",
      "(Iter 100). Current train: 0.843750, Current test: 0.875000.\n",
      "(Iter 110). Current train: 0.921875, Current test: 0.929688.\n",
      "(Iter 120). Current train: 0.890625, Current test: 0.929688.\n",
      "(Iter 130). Current train: 0.937500, Current test: 0.867188.\n",
      "(Iter 140). Current train: 0.937500, Current test: 0.914062.\n",
      "(Iter 150). Current train: 0.937500, Current test: 0.890625.\n",
      "(Iter 160). Current train: 0.921875, Current test: 0.898438.\n",
      "(Iter 170). Current train: 0.859375, Current test: 0.906250.\n",
      "(Iter 180). Current train: 0.953125, Current test: 0.945312.\n",
      "(Iter 190). Current train: 0.859375, Current test: 0.937500.\n",
      "(Iter 200). Current train: 0.937500, Current test: 0.929688.\n",
      "(Iter 210). Current train: 0.921875, Current test: 0.945312.\n",
      "(Iter 220). Current train: 0.984375, Current test: 0.953125.\n",
      "(Iter 230). Current train: 0.984375, Current test: 0.898438.\n",
      "(Iter 240). Current train: 0.968750, Current test: 0.929688.\n",
      "(Iter 250). Current train: 0.968750, Current test: 0.937500.\n",
      "(Iter 260). Current train: 0.937500, Current test: 0.929688.\n",
      "(Iter 270). Current train: 0.937500, Current test: 0.968750.\n",
      "(Iter 280). Current train: 0.875000, Current test: 0.921875.\n",
      "(Iter 290). Current train: 0.937500, Current test: 0.968750.\n",
      "(Iter 300). Current train: 0.937500, Current test: 0.929688.\n",
      "(Iter 0). Current train: 0.109375, Current test: 0.070312.\n",
      "(Iter 10). Current train: 0.187500, Current test: 0.195312.\n",
      "(Iter 20). Current train: 0.453125, Current test: 0.406250.\n",
      "(Iter 30). Current train: 0.546875, Current test: 0.484375.\n",
      "(Iter 40). Current train: 0.578125, Current test: 0.648438.\n",
      "(Iter 50). Current train: 0.734375, Current test: 0.726562.\n",
      "(Iter 60). Current train: 0.828125, Current test: 0.859375.\n",
      "(Iter 70). Current train: 0.796875, Current test: 0.742188.\n",
      "(Iter 80). Current train: 0.812500, Current test: 0.789062.\n",
      "(Iter 90). Current train: 0.859375, Current test: 0.875000.\n",
      "(Iter 100). Current train: 0.828125, Current test: 0.914062.\n",
      "(Iter 110). Current train: 0.875000, Current test: 0.921875.\n",
      "(Iter 120). Current train: 0.906250, Current test: 0.898438.\n",
      "(Iter 130). Current train: 0.921875, Current test: 0.890625.\n",
      "(Iter 140). Current train: 0.921875, Current test: 0.929688.\n",
      "(Iter 150). Current train: 0.953125, Current test: 0.882812.\n",
      "(Iter 160). Current train: 0.843750, Current test: 0.914062.\n",
      "(Iter 170). Current train: 0.921875, Current test: 0.914062.\n",
      "(Iter 180). Current train: 0.921875, Current test: 0.882812.\n",
      "(Iter 190). Current train: 0.937500, Current test: 0.945312.\n",
      "(Iter 200). Current train: 0.937500, Current test: 0.921875.\n",
      "(Iter 210). Current train: 0.921875, Current test: 0.914062.\n",
      "(Iter 220). Current train: 0.921875, Current test: 0.945312.\n",
      "(Iter 230). Current train: 0.921875, Current test: 0.960938.\n",
      "(Iter 240). Current train: 0.875000, Current test: 0.953125.\n",
      "(Iter 250). Current train: 0.953125, Current test: 0.906250.\n",
      "(Iter 260). Current train: 0.968750, Current test: 0.960938.\n",
      "(Iter 270). Current train: 0.937500, Current test: 0.976562.\n",
      "(Iter 280). Current train: 0.921875, Current test: 0.914062.\n",
      "(Iter 290). Current train: 0.953125, Current test: 0.953125.\n",
      "(Iter 300). Current train: 0.906250, Current test: 0.984375.\n",
      "(Iter 0). Current train: 0.156250, Current test: 0.109375.\n",
      "(Iter 10). Current train: 0.171875, Current test: 0.250000.\n",
      "(Iter 20). Current train: 0.515625, Current test: 0.375000.\n",
      "(Iter 30). Current train: 0.546875, Current test: 0.546875.\n",
      "(Iter 40). Current train: 0.703125, Current test: 0.742188.\n",
      "(Iter 50). Current train: 0.734375, Current test: 0.765625.\n",
      "(Iter 60). Current train: 0.765625, Current test: 0.796875.\n",
      "(Iter 70). Current train: 0.781250, Current test: 0.890625.\n",
      "(Iter 80). Current train: 0.875000, Current test: 0.859375.\n",
      "(Iter 90). Current train: 0.921875, Current test: 0.835938.\n",
      "(Iter 100). Current train: 0.796875, Current test: 0.835938.\n",
      "(Iter 110). Current train: 0.890625, Current test: 0.875000.\n",
      "(Iter 120). Current train: 0.859375, Current test: 0.937500.\n",
      "(Iter 130). Current train: 0.828125, Current test: 0.929688.\n",
      "(Iter 140). Current train: 0.906250, Current test: 0.890625.\n",
      "(Iter 150). Current train: 0.953125, Current test: 0.906250.\n",
      "(Iter 160). Current train: 0.953125, Current test: 0.945312.\n",
      "(Iter 170). Current train: 0.875000, Current test: 0.914062.\n",
      "(Iter 180). Current train: 0.937500, Current test: 0.937500.\n",
      "(Iter 190). Current train: 0.953125, Current test: 0.937500.\n",
      "(Iter 200). Current train: 0.937500, Current test: 0.937500.\n",
      "(Iter 210). Current train: 0.890625, Current test: 0.953125.\n",
      "(Iter 220). Current train: 0.984375, Current test: 0.914062.\n",
      "(Iter 230). Current train: 0.921875, Current test: 0.921875.\n",
      "(Iter 240). Current train: 0.953125, Current test: 0.960938.\n",
      "(Iter 250). Current train: 0.968750, Current test: 0.968750.\n",
      "(Iter 260). Current train: 0.937500, Current test: 0.921875.\n",
      "(Iter 270). Current train: 0.921875, Current test: 0.945312.\n",
      "(Iter 280). Current train: 0.937500, Current test: 0.945312.\n",
      "(Iter 290). Current train: 0.968750, Current test: 0.937500.\n",
      "(Iter 300). Current train: 0.953125, Current test: 0.984375.\n"
     ]
    }
   ],
   "source": [
    "# Reset computational graph so its fresh\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# load mnist datasetimport os\n",
    "import os\n",
    "import sys\n",
    "dataset_module_path = os.path.abspath(os.path.join('..'))\n",
    "if dataset_module_path not in sys.path:\n",
    "    sys.path.append(dataset_module_path)\n",
    "from dataset import MnistDataset\n",
    "\n",
    "dataset = MnistDataset(64)\n",
    "\n",
    "# tf inputs\n",
    "inputs_placeholder = tf.placeholder(shape=[None,32,32,1], dtype=tf.float32, name=\"input_placeholder\")\n",
    "labels_placeholder = tf.placeholder(shape=[None,10], dtype=tf.float32, name=\"labels_placeholder\")\n",
    "\n",
    "# run our initialization 5 times\n",
    "our_init_results = []\n",
    "our_init_results_test = []\n",
    "for i in range(5):\n",
    "    with tf.variable_scope(\"scopeee\", reuse=tf.AUTO_REUSE):\n",
    "        scores = make_identity_init_mnist_network(inputs_placeholder, labels_placeholder)\n",
    "        accuracy_results = evaluate(scores, inputs_placeholder, labels_placeholder, dataset)\n",
    "        our_init_results.append(accuracy_results[0]) # train_acc\n",
    "        our_init_results_test.append(accuracy_results[1]) # test_acc\n",
    "\n",
    "# run he initialization 5 times\n",
    "he_init_results = []\n",
    "he_init_results_test = []\n",
    "for i in range(5):\n",
    "    with tf.variable_scope(\"scopeeeee\", reuse=tf.AUTO_REUSE):\n",
    "        scores = make_he_init_mnist_network(inputs_placeholder, labels_placeholder)\n",
    "        accuracy_results = evaluate(scores, inputs_placeholder, labels_placeholder, dataset)\n",
    "        he_init_results.append(accuracy_results[0]) # train_acc\n",
    "        he_init_results_test.append(accuracy_results[1]) # test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs231n/lib/python3.6/site-packages/seaborn/timeseries.py:183: UserWarning: The tsplot function is deprecated and will be removed or replaced (in a substantially altered version) in a future release.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f17bc4f7e80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f17bfe29e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results of the above tests with confidence intervals5,\n",
    "# time = np.zeros((len(our_init_results),len(our_init_results[0])))\n",
    "# time[:] = np.array(list(range(30)) * 10)\n",
    "# print(time)\n",
    "time = np.array(range(31)) * 10\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Mnist Training Set')\n",
    "sns.tsplot(data=our_init_results, time=time, condition=\"Our Init\")\n",
    "sns.tsplot(data=he_init_results, time=time, color=\"g\", condition=\"He Init\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Mnist Test Set')\n",
    "sns.tsplot(data=our_init_results_test, time=time, condition=\"Our Init\")\n",
    "sns.tsplot(data=he_init_results_test, time=time, color=\"g\", condition=\"He Init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: Make the plots above nice with seaborn instead of matplot lib (repeat the code, maybe plot it with mean + stddevs)\n",
    "# TODO 5: Repeat the above with CIFAR-10 (write a dataset object)\n",
    "# TODO 6: Write the expanding code (new section) (write up at the start)\n",
    "# TODO 7: test the split hyperparam\n",
    "    \"\"\"\n",
    "        1. zero init (full capacity, no expandsion)\n",
    "        2. normal init (full capacity, nor expansion)\n",
    "        3. expand at training step (my way) at say [1000, 2000, 3000,...] (n.b. test1 = split at training step 0)\n",
    "        4. expand at training step (random init) at same timesteps (n.b. not function preserving). (n.b. test2 = split at training step 0)\n",
    "        5. Plot all of 1,2,3,4 on the same graph\n",
    "    \"\"\"\n",
    "# load cifar-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of how identity initialized resnet modules perform compared to normal random initializations\n",
    "\n",
    "TODO: Write this up properly + move the code down here for plotting\n",
    "\n",
    "1. Replicate the networks used above to classify mnist and cifar-10, in tf, with normal initializations\n",
    "2. Compare and print out the training curvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working out how to easily alter the computation graph, with low memory cost\n",
    "\n",
    "Here we have a few aims. We want to work out how to replace a kernel withsay $C_1$ filters, by one with $C_2$ filters, but without having to change any of the upstream operations. This following cell we just a space to play with operations, such as tf.assign, to see how this might be able to be performed. Secondly, we work out if we need to do anything to garbage collect within the computational graph, as we'll need to do that a lot (and will be essential for good performance, and not quickly running out of memory).\n",
    "\n",
    "TODO: Add a diagram into this (of how this would work in an R2R module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing variables from FIRST small computational graph:\n",
      "(4th variable) conv1_filter:0\n",
      "(4th variable) Variable:0\n",
      "Result from output sum: 19\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Printing variables from SECOND small computational graph:\n",
      "(4th variable) conv1_filter:0\n",
      "(4th variable) Variable:0\n",
      "(4th variable) conv2_filter:0\n",
      "Result from output sum: 38\n",
      "\n",
      "\n",
      "\n",
      "Printing variables after trying to garbage college old tf variables:\n",
      "(4th variable) conv1_filter:0\n",
      "(4th variable) Variable:0\n",
      "(4th variable) conv2_filter:0\n",
      "\n",
      "\n",
      "\n",
      "Printing variables after using subgraph to purge:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'node'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-98e8831d6b03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\\nPrinting variables after using subgraph to purge:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m#tf.graph_util.extract_sub_graph(tf.get_default_graph(), out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_training_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0mprintvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py\u001b[0m in \u001b[0;36mremove_training_nodes\u001b[0;34m(input_graph, protected_nodes)\u001b[0m\n\u001b[1;32m    315\u001b[0m   \u001b[0mtypes_to_remove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"CheckNumerics\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m   \u001b[0minput_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m   \u001b[0mnames_to_remove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'node'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# helper to print all of the variables out\n",
    "def printvars():\n",
    "    tf_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    for var in tf_vars:\n",
    "        print(\"(%dth variable) %s\" % (i, var.name))\n",
    "\n",
    "# input [1,3,3,1]\n",
    "inputs = tf.constant([1.0,2.0,3.0,\n",
    "                      2.0,3.0,2.0,\n",
    "                      3.0,2.0,1.0], dtype=tf.float32, shape=[1,3,3,1])\n",
    "\n",
    "# 1x1 conv, \"reduction\" down to  [1,3,3,1]\n",
    "filter_init = np.ones((1,1,1,1))\n",
    "conv_filtr = tf.Variable(filter_init, dtype=tf.float32, name=\"conv1_filter\")\n",
    "conv = tf.nn.conv2d(input = inputs,\n",
    "                    filter = conv_filtr,\n",
    "                    strides = [1,1,1,1],\n",
    "                    padding = \"SAME\")\n",
    "\n",
    "# tf variable, what we're going to use to \n",
    "tf_var = tf.Variable(np.zeros((1,3,3,1)), dtype=tf.float32)\n",
    "assign_op = tf.assign(tf_var, conv)\n",
    "\n",
    "# sum output\n",
    "out = tf.reduce_sum(tf_var)\n",
    "\n",
    "# print out the variables that we've made, and then run the result\n",
    "print(\"Printing variables from FIRST small computational graph:\")\n",
    "printvars()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(assign_op)\n",
    "out_val_1 = sess.run(out)\n",
    "print(\"Result from output sum: %d\" % out_val_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now, make a [1,3,3,2] input. Use a 1x1 conv again (filter now with two input channels and one out)\n",
    "# to make shape consistent at [1,3,3,1]\n",
    "# New input (two layers of the same)\n",
    "n_inputs = tf.constant([1.0,2.0,3.0,\n",
    "                        2.0,3.0,2.0,\n",
    "                        3.0,2.0,1.0,\n",
    "                        1.0,2.0,3.0,\n",
    "                        2.0,3.0,2.0,\n",
    "                        3.0,2.0,1.0], dtype=tf.float32, shape=[1,3,3,2])\n",
    "\n",
    "# the new conv\n",
    "n_filter_init = np.ones((1,1,2,1))\n",
    "n_conv_filtr = tf.Variable(n_filter_init, dtype=tf.float32, name=\"conv2_filter\")\n",
    "n_conv = tf.nn.conv2d(input = n_inputs,\n",
    "                      filter = n_conv_filtr,\n",
    "                      strides = [1,1,1,1],\n",
    "                      padding = \"SAME\")\n",
    "\n",
    "# Now, make anop to RE-assign tf_var to be n_conv. \n",
    "reassign_op = tf.assign(tf_var, n_conv)\n",
    "\n",
    "# Print out varialbes in the computation graph now\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Printing variables from SECOND small computational graph:\")\n",
    "printvars()\n",
    "\n",
    "# Run the output op. If we successfully changed the input to sum, this second output should be DOUBLE the first\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(reassign_op)\n",
    "out_val_2 = sess.run(out)\n",
    "print(\"Result from output sum: %d\" % out_val_2)\n",
    "\n",
    "# Unfortunately, we haven't garbage collected the convolution that's no longer part of the output up\n",
    "# Lets remove our references to the (now unused) part of the of the computational graph, and see what happens\n",
    "print(\"\\n\\n\\nPrinting variables after trying to garbage college old tf variables:\")\n",
    "inputs = None\n",
    "conv_filtr = None\n",
    "conv = None\n",
    "assign_op = None\n",
    "\n",
    "printvars()\n",
    "\n",
    "# It turns out, we can use some graph utils, extracting a subgraph, to purge all of the nodes we don't\n",
    "# need in the computational graph\n",
    "print(\"\\n\\n\\nPrinting variables after using subgraph to purge:\")\n",
    "tf.graph_util.extract_sub_graph(tf.get_default_graph(), out)\n",
    "printvars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo list for next time:\n",
    "1. net2widen operations and net2deepen operations\n",
    "\n",
    " 1.1. Requires us to define an resnet v2. We need to be able to initialize the normal layers in the same way as the zero initialized layers (so that we can extend/widen them)\n",
    "2. run, random init, vs random student (init extensions randomly) vs student (function preserving)\n",
    "3. print graphs\n",
    "4. run this on mnist, cifar 10\n",
    "5. construct inception networks\n",
    "\n",
    " 5.1. Requires writing v3 of all of these modules.\n",
    "6. run on imagenet..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
